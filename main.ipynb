{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and evaluation\n",
    "\n",
    "This notebook includes the model training and evaluation for DeepLabv3 with ResNet101 and MobileNetV3 backbone layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "from utils import (\n",
    "    get_device, pixel_accuracy, iou_score, dice_score,\n",
    "    f1_score, batch_files, files_to_tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing dataset\n",
    "\n",
    "Data we use consist of POEM images and a corresponding 2D tensor comprising of the each pixel's label. The images are imported from a local directory and the tensors are store in a pickled dictionary containing the image file name as keys and tensors as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dir = \"/Users/naman/Workspace/Data/BM5020-POEM/Annotated\"\n",
    "\n",
    "pkl_path = f\"{im_dir}/annotations.pkl\"\n",
    "# pkl_path = \"segmented_images.pkl\"\n",
    "\n",
    "with open(pkl_path, \"rb\") as file:\n",
    "    data_dict = pickle.load(file)\n",
    "\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = list(data_dict.keys())\n",
    "\n",
    "batch_size = 4\n",
    "batched_files = batch_files(all_files, batch_size)\n",
    "\n",
    "batched_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the models\n",
    "\n",
    "We load the models with classifier head as DeepLabv3 with 2 different backbones: Resnet101 and MobileNetV3. The classifier head in our model is used to classify each pixel and the backbone layer is used for feature extraction. Backbone layers are initialised with pretrained weights whereas the classifier head is initialised with random weights and 4 number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 4\n",
    "\n",
    "deeplabv3_weights = models.segmentation.DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1\n",
    "resnet101_weights = models.ResNet101_Weights.IMAGENET1K_V2\n",
    "mobilenetv3_weights = models.mobilenet.MobileNet_V3_Large_Weights.IMAGENET1K_V2\n",
    "\n",
    "deeplabv3_resnet101 = models.segmentation.deeplabv3.deeplabv3_resnet101(\n",
    "    num_classes=classes, weights_backbone=resnet101_weights\n",
    ").to(device)\n",
    "deeplabv3_mobilenetv3 = models.segmentation.deeplabv3.deeplabv3_mobilenet_v3_large(\n",
    "    num_classes=classes, weights_backbone=mobilenetv3_weights\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabv3_resnet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabv3_mobilenetv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the optimizer and scheduler\n",
    "\n",
    "We use the Adam optimizer with exponential scheduling. $\\gamma$ in our scheduler is the factor multiplied after each step of the scheduler (which is taken after every epoch). Hence, the learning rate at $i\\text{th}$ epoch will be $\\text{initial\\_lr} * \\gamma^i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 1e-2\n",
    "optimizer = torch.optim.Adam(deeplabv3_mobilenetv3.parameters(), lr=initial_lr)\n",
    "\n",
    "gamma = 0.8\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "accuracy_list = []\n",
    "iou_list = [[] for _ in range(classes)]\n",
    "dice_list = [[] for _ in range(classes)]\n",
    "f1_list = [[] for _ in range(classes)]\n",
    "\n",
    "lr_vals = []\n",
    "\n",
    "epochs = 40\n",
    "batches = len(batched_files)\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    epoch_iou_list = [0] * classes\n",
    "    epoch_dice_list = [0] * classes\n",
    "    epoch_f1_list = [0] * classes\n",
    "\n",
    "    for batch in batched_files:\n",
    "\n",
    "        inputs, labels = files_to_tensors(batch, im_dir, data_dict)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        output = deeplabv3_mobilenetv3(inputs)\n",
    "\n",
    "        logits = output[\"out\"]\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predictions = logits.argmax(dim=1).cpu()\n",
    "        labels = labels.cpu()\n",
    "        epoch_accuracy += pixel_accuracy(predictions, labels)\n",
    "\n",
    "        for i in range(classes):\n",
    "            epoch_iou_list[i] += iou_score(predictions == i, labels == i)\n",
    "            epoch_dice_list[i] += dice_score(predictions == i, labels == i)\n",
    "            epoch_f1_list[i] += f1_score(predictions == i, labels == i)\n",
    "\n",
    "    lr_val = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "    lr_vals.append(lr_val)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} average loss: {epoch_loss / batches}\")\n",
    "\n",
    "    loss_list.append(epoch_loss / batches)\n",
    "    accuracy_list.append(epoch_accuracy / batches)\n",
    "    for i in range(classes):\n",
    "        iou_list[i].append(epoch_iou_list[i] / batches)\n",
    "        dice_list[i].append(epoch_dice_list[i] / batches)\n",
    "        f1_list[i].append(epoch_f1_list[i] / batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Train loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lr_vals)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracy_list)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Pixel accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_ in range(classes):\n",
    "    plt.plot(iou_list[class_], label=f\"Class {class_ + 1}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(f\"IoU score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_ in range(classes):\n",
    "    plt.plot(dice_list[class_], label=f\"Class {class_ + 1}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(f\"Dice score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_ in range(classes):\n",
    "    plt.plot(f1_list[class_], label=f\"Class {class_ + 1}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(f\"F1 score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
